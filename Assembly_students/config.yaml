# Configuration YAML for AssemblyGymEnv training

env:
  # Environment parameters
  max_blocks: 3            # Maximum number of blocks per episode
  n_offsets: 7               # Discrete offset divisions per face
  limit_steps: 100  
  end_reward : 15           # Reward for reaching the goal with max_blocks
  n_floor: 0                   # Number of initial floor blocks
  target_reward_per_block: 5 # Reward per target reached beyond threshold
  min_block_reach_target: 0   # Blocks placed before target rewards start
  collision_penalty: 0     # Penalty added on collision invalid action
  unstable_penalty: 0       # Penalty added on unstable invalid action
  not_reached_penalty: 30   # Penalty for not reaching a goal at episode end

env_wrappers:
  n_envs: 8                    # Number of parallel training environments
  eval_envs: 1                 # Number of evaluation environments
  seed: 42                     # Random seed base

ppo:
  # Stable-Baselines3 PPO hyperparameters
  policy: CnnPolicy
  learning_rate: 1e-4
  n_steps: 1024
  batch_size: 1024
  n_epochs: 10
  clip_range: 0.2
  gamma: 0.99
  ent_coef: 0.02
  target_kl: 0.05
  device: auto                # Auto-select GPU/CPU
  total_timesteps: 500_000    # Total training time steps
  custom_cnn: true            # Enable custom CNN
  #action_dims: [3, 4, 2, 4, 7]  # Example: [target_block, target_face, shape_idx, face, offset_idx]

evaluation:
  eval_freq: 500             # eval_freq = total_timesteps / (n_envs * something)
  deterministic: true
  log_path: "logs/eval_results"
  best_model_save_path: "logs/best_model"

checkpoint:
  save_freq: 1000
  save_path: "logs/checkpoints"
  name_prefix: "ppo_block"

goals:
  # Gaussian reward feature smoothing
  sigma_x: 1.0
  sigma_y: 1.0

task:
  type: StochasticBridge  # Task type   
  args:     # Task constructor from tasks.py
    num_stories: 1      # number of stories for Bridge task

